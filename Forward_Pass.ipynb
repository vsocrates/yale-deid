{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16e38d8",
   "metadata": {},
   "source": [
    "## STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3538a43",
   "metadata": {},
   "source": [
    "* We go through the 4 steps that are required to de-identify a dataset (i.e run the forward pass on this dataset using a trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec21fa5-d545-4999-9127-c21cc3cdfd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e09d475",
   "metadata": {},
   "source": [
    "## STEP 0: LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057cc4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fda2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad9bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robust_deid.ner_datasets import DatasetCreator\n",
    "from robust_deid.sequence_tagging import SequenceTagger\n",
    "from robust_deid.sequence_tagging.arguments import (\n",
    "    ModelArguments,\n",
    "    DataTrainingArguments,\n",
    "    EvaluationArguments,\n",
    ")\n",
    "from robust_deid.deid import TextDeid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4b302",
   "metadata": {},
   "source": [
    "## STEP 1: INITIALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73397b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We require two inputs and all other files are created relative to the input_file\n",
    "\n",
    "# Initialize the path where the dataset is located (input_file).\n",
    "# Input dataset\n",
    "input_file = \"/home/vs428/project/Data/GPT/notes_abd_simple_trigger.csv\"\n",
    "# we also take in a prefix that is appended to all the temporary files created\n",
    "prefix = \"patient\"\n",
    "\n",
    "# we create a \"temp\" folder in the parent folder of your input_file and put all the intermediate steps there.\n",
    "# the final file is output into the parent directory\n",
    "path = Path(input_file)\n",
    "intermediate_path = (path.parent.absolute() / \"temp\")\n",
    "if not os.path.exists(intermediate_path):\n",
    "    os.makedirs(intermediate_path)\n",
    "    \n",
    "# Initialize the location where we will store the sentencized and tokenized dataset (ner_dataset_file)\n",
    "ner_dataset_file = intermediate_path / f'{prefix}_ner.jsonl'\n",
    "\n",
    "# Initialize the location where we will store the model predictions (predictions_file)\n",
    "# Verify this file location - Ensure it's the same location that you will pass in the json file\n",
    "# to the sequence tagger model. i.e. output_predictions_file in the json file should have the same\n",
    "# value as below\n",
    "predictions_file = intermediate_path / f'{prefix}_pred.jsonl'\n",
    "\n",
    "# Initialize the file that will contain the original note text and the de-identified note text\n",
    "deid_file = intermediate_path / f'{prefix}_deid.jsonl'\n",
    "\n",
    "# Initialize the file that will contain the post-processed de-identified note text\n",
    "# NOTE: final file is output to the parent directory instead\n",
    "postprocessed_deid_file = path.parent.absolute() / f'{path.stem}_postprocessed.csv'\n",
    "\n",
    "# Initialize the model config. This config file contains the various parameters of the model.\n",
    "model_config = './config/predict_i2b2.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4938b0-89b7-4477-a268-7b305a61d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it cannot be anything other than 'text' because this package is made badly\n",
    "# I'm assuming this is also the case for note_id\n",
    "text_col_name = \"text\"\n",
    "id_col_name = 'note_id'\n",
    "# the columns you want to include as metadata\n",
    "# NOTE: you will get an error if these columns don't exist in the data\n",
    "meta_cols = ['note_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637e021",
   "metadata": {},
   "source": [
    "# Step 1a: Preprocess with regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb591b-f2e1-4b9e-9c41-58c3f8d8bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df = notes_df.rename({\"0\":\"text\", \"Unnamed: 0\":\"note_id\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time is usually 4 numbers surrounded by spaces, so replace it with <<TIME:1324>>, or it could be XX:XX:XX, or 3:15 AM?\n",
    "notes_df[text_col_name] = notes_df[text_col_name].str.replace(r\"(\\s[0-9]{4}\\s?|[0-9]{2}:[0-9]{2}:[0-9]{2}|[0-9]{1}:[0-9]{2}\\s?([AaPp][Mm])?)\", \n",
    "                             r\"<<TIME:\\1>>\", regex=True)\n",
    "    \n",
    "# we missed provider names sometimes and they were the only thing on the line, ending with MD,PA,PA-C, APRN\n",
    "notes_df[text_col_name] = notes_df[text_col_name].str.replace(r\"^([a-zA-Z,]+?MD|^[a-zA-Z,]+?PA|^[a-zA-Z,]+?PA-C|^[a-zA-Z,]+?APRN)\", \n",
    "                             r\"<<STAFF:\\1>>\", regex=True)\n",
    "\n",
    "\n",
    "# replace address that's often missed\n",
    "# 150 Sargent Dr  New Haven CT 06511-6100\n",
    "notes_df[text_col_name] = notes_df[text_col_name].str.replace(r\"(150 Sargent\\s+?Dr\\s+?New Haven CT\\s+?06511-6100)\", \n",
    "                             r\"<<LOCATION:\\1>>\", regex=True)\n",
    "\n",
    "# sometimes numbers are missed of type XXX.XXX.XXX or XXX-XXX-XXXX\n",
    "notes_df[text_col_name] = notes_df[text_col_name].str.replace(r\"([0-9]{3}\\.[0-9]{3}\\.[0-9]{4}|[0-9]{3}\\-[0-9]{3}\\-[0-9]{4})\", \n",
    "                             r\"<<PHONE:\\1>>\", regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459492f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 1b: Convert CSV to the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b69085",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df['meta'] = json.loads(notes_df[meta_cols].to_json(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42527d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df['spans'] = notes_df.shape[0] * [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_fp = str(intermediate_path / path.stem) + \"_preprocessed\" + \".jsonl\"\n",
    "notes_df[[text_col_name, \"meta\", \"spans\"]].to_json(preprocessed_fp, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22608857",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac04b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## STEP 2: NER DATASET\n",
    "* Sentencize and tokenize the raw text. We used sentences of length 128, which includes an additional 32 context tokens on either side of the sentence. These 32 tokens serve (from the previous & next sentence) serve as additional context to the current sentence.\n",
    "* We used the en_core_sci_lg sentencizer and a custom tokenizer (can be found in the preprocessing module)\n",
    "* The dataset stored in the ner_dataset_file will be used as input to the sequence tagger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b2277-3d5e-4398-8e59-68bae3a0d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset creator object\n",
    "\n",
    "dataset_creator = DatasetCreator(\n",
    "    sentencizer='en_core_sci_sm',\n",
    "    tokenizer='clinical',\n",
    "    max_tokens=128,\n",
    "    max_prev_sentence_token=32,\n",
    "    max_next_sentence_token=32,\n",
    "    default_chunk_size=32,\n",
    "    ignore_label='NA'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff148fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9bc25a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function call sentencizes and tokenizes the dataset\n",
    "# It returns a generator that iterates through the sequences.\n",
    "# We write the output to the ner_dataset_file (in json format)\n",
    "ner_notes = dataset_creator.create(\n",
    "    input_file=preprocessed_fp,\n",
    "    mode='predict',\n",
    "    notation='BILOU',\n",
    "    token_text_key=text_col_name,\n",
    "    metadata_key='meta',\n",
    "    note_id_key=id_col_name,\n",
    "    label_key='label',\n",
    "    span_text_key='spans'\n",
    ")\n",
    "# Write to file\n",
    "with open(ner_dataset_file, 'w') as file:\n",
    "    for ner_sentence in ner_notes:\n",
    "        file.write(json.dumps(ner_sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16054cff",
   "metadata": {},
   "source": [
    "## STEP 3: SEQUENCE TAGGING\n",
    "* Run the sequence model - specify parameters to the sequence model in the config file (model_config). The model will be run with the specified parameters. For more information of these parameters, please refer to huggingface (or use the docs provided).\n",
    "* This file uses the argmax output. To use the recall threshold models (running the forward pass with a recall biased threshold for aggressively removing PHI) use the other config files.\n",
    "* The config files in the i2b2 direct`ory specify the model trained on only the i2b2 dataset. The config files in the mgb_i2b2 directory is for the model trained on both MGB and I2B2 datasets.\n",
    "* You can manually pass in the parameters instead of using the config file. The config file option is recommended. In our example we are passing the parameters through a config file. If you do not want to use the config file, skip the next code block and manually enter the values in the following code blocks. You will still need to read in the training args using huggingface and change values in the training args according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((\n",
    "    ModelArguments,\n",
    "    DataTrainingArguments,\n",
    "    EvaluationArguments,\n",
    "    TrainingArguments\n",
    "))\n",
    "# If we pass only one argument to the script and it's the path to a json file,\n",
    "# let's parse it to get our arguments.\n",
    "model_args, data_args, evaluation_args, training_args = parser.parse_json_file(json_file=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb8138",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the sequence tagger\n",
    "sequence_tagger = SequenceTagger(\n",
    "    task_name=data_args.task_name,\n",
    "    notation=data_args.notation,\n",
    "    ner_types=data_args.ner_types,\n",
    "    model_name_or_path=model_args.model_name_or_path,\n",
    "    config_name=model_args.config_name,\n",
    "    tokenizer_name=model_args.tokenizer_name,\n",
    "    post_process=model_args.post_process,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    model_revision=model_args.model_revision,\n",
    "    use_auth_token=model_args.use_auth_token,\n",
    "    threshold=model_args.threshold,\n",
    "    do_lower_case=data_args.do_lower_case,\n",
    "    fp16=training_args.fp16,\n",
    "    seed=training_args.seed,\n",
    "    local_rank=training_args.local_rank\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9bd177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required functions of the sequence tagger\n",
    "sequence_tagger.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb116ee-15d2-4b18-87a7-8a548934c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the required data and predictions of the sequence tagger\n",
    "# Can also use data_args.test_file instead of ner_dataset_file (make sure it matches ner_dataset_file)\n",
    "sequence_tagger.set_predict(\n",
    "    test_file=str(ner_dataset_file),\n",
    "    max_test_samples=data_args.max_predict_samples,\n",
    "    preprocessing_num_workers=data_args.preprocessing_num_workers,\n",
    "    overwrite_cache=data_args.overwrite_cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe41f2-f5bc-42f0-b231-b951b25b28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: we use the `eval_accumulation_steps` in the config file so that we don't try and gather the entire evaluation dataset on the GPU after evaluation. \n",
    "# This helps ensure that we don't hit any GPU CUDA errors. Instead, only play with the batch size. \n",
    "\n",
    "training_args.fp16 = True\n",
    "training_args.disable_tqdm = False\n",
    "training_args.fp16_full_eval = True\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa70c49",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the huggingface trainer\n",
    "sequence_tagger.setup_trainer(training_args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions in the specified file\n",
    "predictions = sequence_tagger.predict()\n",
    "# Write predictions to a file\n",
    "with open(predictions_file, 'w') as file:\n",
    "    for prediction in predictions:\n",
    "        file.write(json.dumps(prediction) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e214964",
   "metadata": {},
   "source": [
    "## STEP 4: DE-IDENTIFY TEXT\n",
    "\n",
    "* This step uses the predictions from the previous step to de-id the text. We pass the original input file where the original text is present. We look at this text and the predictions and use both of these to de-id the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text deid object\n",
    "text_deid = TextDeid(notation='BILOU', span_constraint='super_strict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e56c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-identify the text - using deid_strategy=replace_informative doesn't drop the PHI from the text, but instead\n",
    "# labels the PHI - which you can use to drop the PHI or do any other processing.\n",
    "# If you want to drop the PHI automatically, you can use deid_strategy=remove\n",
    "deid_notes = text_deid.run_deid(\n",
    "    input_file=preprocessed_fp,\n",
    "    predictions_file=str(predictions_file),\n",
    "    deid_strategy='replace_informative',\n",
    "    keep_age=False,\n",
    "    metadata_key='meta',\n",
    "    note_id_key=id_col_name,\n",
    "    tokens_key='tokens',\n",
    "    predictions_key='predictions',\n",
    "    text_key=text_col_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the deidentified output to a file\n",
    "with open(deid_file, 'w') as file:\n",
    "    for deid_note in deid_notes:\n",
    "        file.write(json.dumps(deid_note) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = time.time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16699d",
   "metadata": {},
   "source": [
    "# Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c1a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "deid_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deid_notes = []\n",
    "with jsonlines.open(deid_file) as reader:\n",
    "    for line in reader:\n",
    "        deid_notes.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4099d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deid_df = pd.DataFrame.from_records(deid_notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex rules\n",
    "\n",
    "# first clean up the accidental DE-ID from the pipeline in the template for the HEART score\n",
    "# need to replace it with the original string\n",
    "deid_df['deid_text'] = deid_df['deid_text'].str.replace(r\"<<AGE:45 - 64>>\", \"45 - 64\", regex=True)\n",
    "\n",
    "deid_df['deid_text'] = deid_df['deid_text'].str.replace(r\"- < <<AGE:45>> 0\", \"- < 45 0\", regex=True)\n",
    "\n",
    "# replace incomplete hospital name acrnoyms\n",
    "# <<HOSPITAL:BH>> GH LMW Q YH -> <<HOSPITAL:BH GH LMW Q YH>>\n",
    "deid_df['deid_text'] = deid_df['deid_text'].str.replace(r\"<<HOSPITAL:BH>> GH LMW Q YH\", \"<<HOSPITAL:BH GH LMW Q YH>>\", regex=True)\n",
    "\n",
    "# replace incomplete yale name\n",
    "# <<HOSPITAL:Yale>> Radiology and Biomedical Imaging -> <<HOSPITAL:Yale Radiology and Biomedical Imaging>>\n",
    "deid_df['deid_text'] = deid_df['deid_text'].str.replace(r\"<<HOSPITAL:Yale>> Radiology and Biomedical Imaging\", \n",
    "                                 r\"<<HOSPITAL:Yale Radiology and Biomedical Imaging>>\", regex=True)\n",
    "                                                                                            \n",
    "#############################################\n",
    "#############################################\n",
    "#############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deid_df.to_csv(postprocessed_deid_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10ee44-5ef0-403a-88eb-6a6a856f9599",
   "metadata": {},
   "source": [
    "# Drop all Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde9bb1-401f-4673-b333-c29fd13ab493",
   "metadata": {},
   "outputs": [],
   "source": [
    "deid_df = pd.read_csv(postprocessed_deid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1aba71-a271-47e5-8189-de98ecb62995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def drop_deid_tags(df, col):\n",
    "    df[col + \"_replaced\"] = df[col].str.replace(r\"<<([A-Z]+?):.*?>>\",  r'<<\\1>>', regex=True, flags=re.DOTALL)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588572a-081f-48dc-8625-837c0c9c485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced = drop_deid_tags(deid_df, \"deid_text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3040a8-0035-4db7-a4fe-3c2d7a81b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "replaced['meta'] = replaced['meta'].apply(lambda x: ast.literal_eval(x))\n",
    "replaced = pd.concat([replaced.drop(['meta'], axis=1), replaced['meta'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1700ce1-b5b7-4052-aca0-bce9e2b0d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a2cac-1f48-46d0-bf64-a9af4e61b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced.to_csv(postprocessed_deid_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a208e70-df72-4514-97d7-9ac0b07144ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8cdcd-5647-43a3-88d2-446be43916bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c4f56-73d3-4c88-8c1d-3bfe2cc955a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c54a3-2b4f-4921-8034-5771a0279a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7268e-8be7-4354-abae-aa1adacb9779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
